Software Development Individual Assignment 2
Cloud: https://weatherdashboard-fairouz-container-api-g7cxfuckcycbcqae.westeurope-01.azurewebsites.net
Fairouz Abou Mousa

Introduction
The assignment goal was to convert a simple Flask weather app into a production-quality service by applying DevOps principles and best practices in software engineering. During the project, I employed among others, modular design, unit and integration testing, continuous integration with GitHub Actions, Docker-based containerization, cloud deployment via Azure Web App for Containers, and service observability using health checks and Prometheus monitoring. This conversion was not just moving from a simple local script to a cloud application that is fully deployed and monitored but it was also a careful redesigning of the architecture, automation of major development tasks, and a deep understanding of how the different DevOps tools and practices could be combined to create a reliable software system.

The report under this document gives a detailed description of the project's development. It tells the story of how the initial code was refactored into a clean architecture, how the correctness and reliability were ensured through testing, how continuous integration kept the code quality high, how Docker made the app portable, how the app was deployed to Azure using an image in the Azure Container Registry, and lastly, how the health checks and the metrics-based monitoring were set up to help in the maintainability of the application in the production environment. The report reflects the entire learning curve and the technical decisions that were pivotal for the transformation of a straightforward app into a cloud-ready, maintainable, and observable service.

Code Quality Improvements 
2.1 Refactoring into a clean architecture 
At the project's start, the Flask application was a one-script solution with almost the entire logic in it. This kind of structure not only made the application hard to test but also made it difficult to extend and very susceptible to bugs. To solve these problems, I reorganized the whole project based on a more maintainable architecture that was inspired by the Single Responsibility Principle. The new structure makes a clear division of concerns by having separate modules for each. The app.py file has now become only an application factory: it creates and configures the Flask app, registers routes, and sets up middleware. The whole weather-retrieval logic was relocated to utils/api_client.py, which is the one that communicates with the external OpenWeatherMap API. The persistence logic, which includes loading and saving favorites and search history, is now under utils/storage.py. All the HTTP routing takes place in main/routes.py through the usage of a Flask Blueprint, which guarantees that the request handling is independent of the application setup. Lastly, the operational monitoring logic was moved to utils/metrics.py, where the custom counters and latency metrics for Prometheus monitoring are managed. This division of tasks not only makes the code more readable but also allows the individual components to be tested and maintained more easily in isolation.

2.2 Test Suite: Unit and integration Testing
A powerful testing strategy is what made it possible to depend on the newly refactored code during the deployment phase. To realize this, I worked on and wrote a complete package of unit and integration tests using pytest, pytest-mock, and requests-mock. By executing the unit tests, one observes the performance of solo parts without any external effects. To illustrate it, clients of the API were tested, and I imitated the reception of responses from OpenWeatherMap such as, for instance, successful lookups and error conditions like invalid cities or no service available. All this led to the conclusion that the application would react with composure even in the worst of circumstances. In like manner, the storage module was put through the wringer by way of temporary JSON files, thus pacifying the logic for favorite addition, search history holding, and retrieved data behaving exactly as one wants it to be without even touching the real production data file.

The integration tests were carried out with the help of Flask’s test client, which acted as a user in the steady flow of interaction. The tests cover complete request-response streams, thus assuring that the application would act right when a page is loaded, a form is submitted, or when a user tries to add or get rid of favorites. By anchoring together the routes, templates, and service interactions, these tests are the ground for the confidence that the application performs correctly as a whole. The testing method is similar to that in professional DevOps pipelines where it is a must for the automated tests to be run upon every code alteration so as to avert regression and to have stable deployments.


2.3 Coverage Reporting 
In order to evaluate the test suite's thoroughness, I used pytest-cov that provides comprehensive coverage reports containing terminal summaries, XML reports, and visual metrics in HTML format among others. The ultimate coverage obtained was around 97%, and very few minor branches usually defensive errors or edge cases were left untested. It is no doubt that this high level of coverage has confirmed the test suite's credibility and its support for safe refactoring, maintainability, and future feature addition.

2.4 Code Smells and How They Were Addressed 
The process of refactoring brought to light numerous code smells in the initial implementation that rendered the app maintenance and evolution difficult. The biggest concern was the “God File” where all the logic was practically stored in a script with routing, external API calls, persistence, and metrics among others. This was against the basic design principles and severely restricted modularity and testability. Dividing the file into different modules each with one responsibility made the architecture clearer and more extensible.

Moreover, additional smells like duplicated logic also reduced the quality of the codebase. Repeated patterns of JSON handling, error-handling blocks and route-level logic, all contributed to creating unnecessary complications. The service classes that were created for these functions helped in eliminating the duplication and thus the risk of inconsistent behaviour was also reduced. Also, the original system had a very tight coupling of routes to direct file operations which made it practically impossible to do unit testing without involving real files. The introduction of service layers countered the problem and made it possible for the tests to mock dependencies in a clean way.

Security risks and deployment challenges were both created by hardcoded values one of which was the API key. Such values were replaced by environment variables allowing proper configurations in cloud environments. Similarly, other issues like uninformative exception handling, unused code, scattered imports, and lack of clear dependency boundaries were also solved through refactoring. Also, the problem of tests overwriting production data was resolved by replacing real file access with temporary files in tests. To sum up, dealing with these code smells was a very significant step that later on enabled successful CI integration, Dockerization, and deployment.

Continuous Integration (CI)
3.1 CI Pipeline Setup With GitHub Actions 
In order to automate the testing process and provide reliable code quality, I set up a comprehensive continuous integration pipeline with the help of GitHub Actions. The pipeline is stored in a workflow file located at .github/workflows/ci.yml, which is automatically triggered on every push or pull request to the main branch. The workflow starts with the checking out of the repository and the Python 3.13 environment setting up. Then, it goes on to install the dependencies of the project as well as all testing tools, for instance, pytest, pytest-cov, and pytest-mock. After the environment is prepared, the workflow runs the complete test suite and produces a coverage report.

3.2 Coverage Enforcement and Build Validation 
Enforcing a minimum coverage threshold was one of the main aspects of the assignment. For that reason, I used a python script that is part of the YAML workflow to read the coverage.xml file and compute the overall test coverage. When the percentage is under 70%, the pipeline fails automatically, and the run gets marked as unsuccessful in GitHub. This way of working is like real CI pipelines in professional software teams, where branches cannot be merged unless coverage is above a defined threshold. Moreover, the pipeline does a quick build validation by trying to import the Flask application. This guarantees no syntax or import errors will prevent deployment. By integrating CI into the codebase, each change is validated automatically which reduces technical debt and the risk of regressions.

Containerization and Deployment 
4.1 Docker-Based Containerication 
Containerization gave the application the ability to run without any differences in the different environments. I made a Dockerfile with python:3.13-slim as the base image, which is incredibly light and perfect for the production environment. The Dockerfile specifies all the steps that are needed: copying the project to the /app directory, installing the required packages, and exposing the port 5000, which is the default for the Flask app and also defined using the CMD instruction. To prevent conflicts with macOS—particularly, the frequent use of port 5000 by AirPlay—port remapping (docker run -p 5001:5000 weatherdashboard) was used for local testing. The container was run to check if all the application routes including the homepage, /health, and /metrics worked as expected.
4.2 Azure Container Deployment via ACR and Web APp for Containers 
In order to get the application deployed on the cloud, it was necessary to set up both the Azure Container Registry (ACR) and the Azure Web App for Containers. I first built the Docker image, assigned it a tag, and then pushed it to the private registry fairouzweatherdashboardregistry with the tag weatherapp:latest. After the image upload, I set up a Linux-based Azure Web App and sewed it to the container registry for the deployment. The application was not configured to run on port 80, so I had to expose port 5001 through the Enhanced Configuration for the Web App. Without this port, Azure would show the common “Application Error” and stop the container from running.

After making the right port and the environment variables, including the WEATHER_API_KEY, the application was able to deploy without any issues. I checked the deployment by continually accessing all endpoints, using the interface, and even noticing that the storage and metrics acted as expected in a cloud environment. The deployment was done manually instead of via a continuous deployment pipeline, but the project is a complete demonstration of a working cloud-hosted containerized application.

Monitoring and Health Checks  
5.1 Health Check Endpoint 
As part of my strategy to guarantee observability and operational reliability, I set up a health endpoint that is very light in weight and is still able to perform its functions. The endpoint gives back a very basic JSON response that shows the status of the application. It is free from any external API calls ensuring its utility for uptime checks as well as for automated health probes in cloud settings. The endpoint can be utilized by Azure Web App and external monitoring tools to check whether the service is operating properly and then take appropriate action.

5.2 Custom Metrics for Prometheus Monitoring 
For the purpose of supporting monitoring driven by metrics, I developed in metrics.py a set of manual metrics that can be used with Prometheus. Rather than using external libraries that led to various compatibility problems, I developed counters for total requests, total errors, and average request latency. These counters are modified via Flask request hooks: before_request, after_request, and errorhandler. The values are made available at the /metrics endpoint in standard Prometheus exposition format, which implies that the application is easily scrapped by any Prometheus server.

5.3 Prometheus Configuration and Validation  
For metrics testing, I performed a local installation of Prometheus and set up a custom prometheus.yml configuration that gave a scrape interval of five seconds. The server was set up to scrape metrics from the endpoint localhost:5001/metrics. After starting Prometheus, I checked the metrics and they were indeed getting updated every moment: the request counts went up after every refresh of the page, the error counts got incremented for every error occurrence, and the latency metric showed changes according to request processing time. Such a monitoring setup is quite analogous to production observability systems and shows the potential of the application being monitored in a live deployment environment.

Conclusion 
This particular task gave me a thorough experience of the complete software engineering and DevOps practices cycle. I started with a very simple and not very structured Flask app, then I redesigned the whole project step by step from the beginning to the end of the process through the application of the proper industry standards. I generated an extensive automated tests suite and incorporated CI with GitHub Actions. Moreover, I dockerized the app, then ran it on Azure Web App for Containers, and finally, I had real monitoring with compatible metrics and health checks done by Prometheus. The end product is a system that is much easier to maintain, more reliable, and more scalable than the original one. It also shows how the different processes of development, testing, deployment, and monitoring collaborate in a real DevOps pipeline, giving a strong base for future projects and personal growth in cloud-based software engineering.


Appendix 
Monitoring Configuration: 
Prometheus: http://localhost:9090/query?g0.expr=weather_request_count&g0.show_tree=0&g0.tab=table&g0.range_input=1h&g0.res_type=auto&g0.res_density=medium&g0.display_mode=lines&g0.show_exemplars=0

Metrics: http://localhost:5001/metrics

Health: http://localhost:5001/health 


